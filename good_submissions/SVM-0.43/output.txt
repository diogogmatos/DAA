â³ Loading datasets...

ğŸª„ Preprocessing datasets...

ğŸ§ Performing Recursive Feature Elimination...
Fitting Random Forest model...
Random Forest Accuracy: 47.54%
Random Forest F1 Score: 0.41
ğŸ§¹ Removed 604 features (New total: 1409).

âš–ï¸ Balancing classes with SMOTE...

ğŸ§ Performing Principal Component Analysis
ğŸ§¹ Reduced 1338 features (New total: 71).

âš™ï¸ Training models...
Fitting Decision Tree model...
Decision Tree Accuracy: 36.07%
Decision Tree F1 Score: 0.31
Fitting Random Forest model...
Random Forest Accuracy: 31.15%
Random Forest F1 Score: 0.28
Fitting XGBoost model...
XGBoost Accuracy: 32.79%
XGBoost F1 Score: 0.32
Fitting SVM model...
SVM Accuracy: 44.26%
SVM F1 Score: 0.43
Fitting Gradient Boosting model...
Gradient Boosting Accuracy: 31.15%
Gradient Boosting F1 Score: 0.30
Grid Searching Bagging model...
Fitting Bagging model...
Bagging Accuracy: 39.34%
Bagging F1 Score: 0.35

ğŸ”€ Stacking top 3 models...
Fitting Stacking model...
/home/diogo/Uni/MEI/1ano/DAA/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Stacking Accuracy: 42.62%
Stacking F1 Score: 0.42

ğŸ“Š Model comparison (F1):
1. Svm: 0.43
2. Stacking: 0.42
3. Bagging: 0.35
4. Xgboost: 0.32
5. Decision tree: 0.31
6. Gradient boosting: 0.3
7. Random forest: 0.28

ğŸ“ Generating submissions...
Generating SVM submission...
Generating stacking submission...
Generating Bagging submission...
Generating XGBoost submission...
Generating Decision Tree submission...
Generating Gradient Boosting submission...
Generating Random Forest submission...

âœ… Done!
